"""
Exemple d'utilisation des neurones à dendrites actives avec ML-Agents.

Ce fichier montre comment intégrer les neurones à dendrites actives
dans un environnement de reinforcement learning multitâche.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import List, Dict, Any

from mlagents_envs.base_env import ActionSpec, ObservationSpec, ObservationType
from mlagents.trainers.settings import NetworkSettings
from mlagents_envs.base_env import DimensionProperty

from .mlagents_integration import (
    ActiveDendritesNetworkBody,
    ActiveDendritesActor,
    ActiveDendritesValueNetwork,
    ContextProvider
)


def create_example_environment():
    """
    Crée un exemple d'environnement multitâche simple.
    
    Returns:
        Tuple contenant les spécifications d'observation et d'action
    """
    # Spécifications d'observation
    observation_specs = [
        # Observation vectorielle (position, vitesse, etc.)
        ObservationSpec(
            shape=(10,),
            observation_type=ObservationType.DEFAULT,
            dimension_property=(DimensionProperty.NONE,)
        ),
        # Observation visuelle (image de la caméra)
        ObservationSpec(
            shape=(3, 64, 64),  # RGB, 64x64
            observation_type=ObservationType.DEFAULT,
            dimension_property=(
                DimensionProperty.NONE,
                DimensionProperty.TRANSLATIONAL_EQUIVARIANCE,
                DimensionProperty.TRANSLATIONAL_EQUIVARIANCE
            )
        )
    ]
    
    # Spécification d'action
    action_spec = ActionSpec(
        continuous_size=4,  # 4 actions continues
        discrete_branches=[],  # Pas d'actions discrètes
        discrete_size=0
    )
    
    return observation_specs, action_spec


def create_network_settings():
    """
    Crée les paramètres de réseau pour les neurones à dendrites actives.
    
    Returns:
        NetworkSettings configuré
    """
    return NetworkSettings(
        normalize=True,
        hidden_units=128,
        num_layers=3,
        vis_encode_type=NetworkSettings.EncoderType.SIMPLE,
        memory=None,  # Pas de mémoire LSTM pour cet exemple
        goal_conditioning_type=NetworkSettings.ConditioningType.NONE,
        deterministic=False
    )


def example_basic_usage():
    """
    Exemple d'utilisation basique des neurones à dendrites actives.
    """
    print("=== Exemple d'utilisation basique ===")
    
    # Créer l'environnement
    observation_specs, action_spec = create_example_environment()
    network_settings = create_network_settings()
    
    # Paramètres des dendrites actives
    context_size = 10
    num_dendritic_segments = 8
    segment_size = 16
    k_wta = 20  # 20 neurones actifs sur 128
    sparsity_ratio = 0.15
    
    # Créer le corps de réseau
    network_body = ActiveDendritesNetworkBody(
        observation_specs=observation_specs,
        network_settings=network_settings,
        context_size=context_size,
        num_dendritic_segments=num_dendritic_segments,
        segment_size=segment_size,
        k_wta=k_wta,
        sparsity_ratio=sparsity_ratio
    )
    
    # Créer le fournisseur de contexte
    context_provider = ContextProvider(context_size=context_size, num_tasks=5)
    
    # Simuler des données d'entrée
    batch_size = 4
    
    # Observations simulées
    vector_obs = torch.randn(batch_size, 10)
    visual_obs = torch.randn(batch_size, 3, 64, 64)
    inputs = [vector_obs, visual_obs]
    
    # Contexte pour différentes tâches
    context_task_0 = context_provider.get_context(batch_size, task_id=0)
    context_task_1 = context_provider.get_context(batch_size, task_id=1)
    
    # Forward pass pour la tâche 0
    encoding_0, memories_0 = network_body(inputs, context=context_task_0)
    print(f"Encodage pour tâche 0: {encoding_0.shape}")
    
    # Forward pass pour la tâche 1
    encoding_1, memories_1 = network_body(inputs, context=context_task_1)
    print(f"Encodage pour tâche 1: {encoding_1.shape}")
    
    # Vérifier que les encodages sont différents
    difference = torch.norm(encoding_0 - encoding_1)
    print(f"Différence entre encodages: {difference.item():.4f}")
    
    # Obtenir les segments actifs
    active_segments = network_body.get_active_segments()
    print(f"Nombre de couches avec segments actifs: {len(active_segments)}")
    
    return network_body, context_provider


def example_actor_critic():
    """
    Exemple d'utilisation avec Actor-Critic.
    """
    print("\n=== Exemple Actor-Critic ===")
    
    # Créer l'environnement
    observation_specs, action_spec = create_example_environment()
    network_settings = create_network_settings()
    
    # Paramètres des dendrites actives
    context_size = 10
    num_dendritic_segments = 8
    segment_size = 16
    k_wta = 20
    sparsity_ratio = 0.15
    
    # Créer l'acteur
    actor = ActiveDendritesActor(
        observation_specs=observation_specs,
        network_settings=network_settings,
        action_spec=action_spec,
        context_size=context_size,
        num_dendritic_segments=num_dendritic_segments,
        segment_size=segment_size,
        k_wta=k_wta,
        sparsity_ratio=sparsity_ratio
    )
    
    # Créer le réseau de valeur
    value_network = ActiveDendritesValueNetwork(
        stream_names=["value"],
        observation_specs=observation_specs,
        network_settings=network_settings,
        context_size=context_size,
        num_dendritic_segments=num_dendritic_segments,
        segment_size=segment_size,
        k_wta=k_wta,
        sparsity_ratio=sparsity_ratio
    )
    
    # Créer le fournisseur de contexte
    context_provider = ContextProvider(context_size=context_size, num_tasks=3)
    
    # Simuler des données
    batch_size = 2
    vector_obs = torch.randn(batch_size, 10)
    visual_obs = torch.randn(batch_size, 3, 64, 64)
    inputs = [vector_obs, visual_obs]
    
    # Contexte pour la tâche 0
    context = context_provider.get_context(batch_size, task_id=0)
    
    # Obtenir l'action et les statistiques
    action, stats, memories = actor.get_action_and_stats(
        inputs, context=context
    )
    
    print(f"Action continue: {action.continuous_tensor.shape}")
    print(f"Log probs: {stats['log_probs'].shape}")
    print(f"Entropies: {stats['entropies'].shape}")
    
    # Évaluer la valeur
    value_outputs, value_memories = value_network.critic_pass(
        inputs, context=context
    )
    
    print(f"Valeurs: {value_outputs['value'].shape}")
    
    return actor, value_network, context_provider


def example_continual_learning():
    """
    Exemple de continual learning avec Synaptic Intelligence.
    """
    print("\n=== Exemple de Continual Learning ===")
    
    # Créer l'environnement
    observation_specs, action_spec = create_example_environment()
    network_settings = create_network_settings()
    
    # Créer l'acteur
    actor = ActiveDendritesActor(
        observation_specs=observation_specs,
        network_settings=network_settings,
        action_spec=action_spec,
        context_size=10,
        num_dendritic_segments=8,
        segment_size=16,
        k_wta=20,
        sparsity_ratio=0.15
    )
    
    # Optimiseur
    optimizer = optim.Adam(actor.parameters(), lr=1e-4)
    
    # Fournisseur de contexte
    context_provider = ContextProvider(context_size=10, num_tasks=3)
    
    # Simuler l'apprentissage sur plusieurs tâches
    num_episodes_per_task = 10
    batch_size = 4
    
    for task_id in range(3):
        print(f"\n--- Apprentissage de la tâche {task_id} ---")
        
        # Définir la tâche actuelle
        context_provider.set_current_task(task_id)
        
        # Sauvegarder les paramètres optimaux avant l'apprentissage
        actor.save_optimal_params()
        
        for episode in range(num_episodes_per_task):
            # Simuler des données d'entraînement
            vector_obs = torch.randn(batch_size, 10)
            visual_obs = torch.randn(batch_size, 3, 64, 64)
            inputs = [vector_obs, visual_obs]
            
            # Contexte pour cette tâche
            context = context_provider.get_context(batch_size, task_id)
            
            # Forward pass
            action, stats, memories = actor.get_action_and_stats(
                inputs, context=context
            )
            
            # Simuler une perte (dans un vrai cas, ce serait la perte RL)
            loss = torch.mean(stats['entropies']) + 0.1 * torch.mean(action.continuous_tensor ** 2)
            
            # Mettre à jour les coefficients d'importance SI
            actor.update_importance_weights(loss)
            
            # Ajouter le terme de régularisation SI
            si_loss = actor.compute_regularization_loss()
            total_loss = loss + 0.1 * si_loss
            
            # Backward pass
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            if episode % 5 == 0:
                print(f"  Episode {episode}: Loss = {total_loss.item():.4f}")
        
        # Afficher les segments actifs pour cette tâche
        active_segments = actor.get_active_segments()
        total_active = sum(torch.sum(seg).item() for layer in active_segments for seg in layer)
        print(f"  Segments actifs pour tâche {task_id}: {total_active}")
    
    print("\nApprentissage terminé!")


def example_visualization():
    """
    Exemple de visualisation des segments actifs.
    """
    print("\n=== Exemple de Visualisation ===")
    
    # Créer un réseau simple pour la visualisation
    observation_specs, action_spec = create_example_environment()
    network_settings = create_network_settings()
    
    actor = ActiveDendritesActor(
        observation_specs=observation_specs,
        network_settings=network_settings,
        action_spec=action_spec,
        context_size=5,
        num_dendritic_segments=4,
        segment_size=8,
        k_wta=10,
        sparsity_ratio=0.1
    )
    
    context_provider = ContextProvider(context_size=5, num_tasks=3)
    
    # Tester avec différentes tâches
    batch_size = 1
    vector_obs = torch.randn(batch_size, 10)
    visual_obs = torch.randn(batch_size, 3, 64, 64)
    inputs = [vector_obs, visual_obs]
    
    print("Segments actifs par tâche:")
    for task_id in range(3):
        context = context_provider.get_context(batch_size, task_id)
        
        # Forward pass
        action, stats, memories = actor.get_action_and_stats(
            inputs, context=context
        )
        
        # Obtenir les segments actifs
        active_segments = actor.get_active_segments()
        
        print(f"Tâche {task_id}:")
        for layer_idx, layer_segments in enumerate(active_segments):
            layer_active = sum(torch.sum(seg).item() for seg in layer_segments)
            print(f"  Couche {layer_idx}: {layer_active} segments actifs")


if __name__ == "__main__":
    """
    Exécute tous les exemples.
    """
    print("Exemples d'utilisation des neurones à dendrites actives")
    print("=" * 60)
    
    try:
        # Exemple basique
        network_body, context_provider = example_basic_usage()
        
        # Exemple Actor-Critic
        actor, value_network, context_provider = example_actor_critic()
        
        # Exemple de continual learning
        example_continual_learning()
        
        # Exemple de visualisation
        example_visualization()
        
        print("\n" + "=" * 60)
        print("Tous les exemples ont été exécutés avec succès!")
        
    except Exception as e:
        print(f"Erreur lors de l'exécution des exemples: {e}")
        import traceback
        traceback.print_exc() 