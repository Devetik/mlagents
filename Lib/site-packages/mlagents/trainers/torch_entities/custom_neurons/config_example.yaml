# Configuration ML-Agents avec neurones à dendrites actives
# Placez ce fichier dans le dossier configs/ de votre projet Unity

default_settings:
  trainer_type: ppo
  hyperparameters:
    batch_size: 64
    buffer_size: 12000
    learning_rate: 0.0003
    beta: 0.001
    epsilon: 0.2
    lambd: 0.95
    num_epoch: 3
    learning_rate_schedule: linear
  network_settings:
    normalize: true
    hidden_units: 128
    num_layers: 3
    vis_encode_type: simple
    memory: null
    goal_conditioning_type: none
    deterministic: false
  # Configuration des neurones à dendrites actives
  use_active_dendrites: true
  context_size: 10
  num_dendritic_segments: 8
  segment_size: 16
  k_wta: 20
  sparsity_ratio: 0.15
  # Paramètres Synaptic Intelligence
  si_coefficient: 0.1
  # Paramètres d'entraînement
  max_steps: 1000000
  time_horizon: 64
  summary_freq: 10000
  keep_checkpoints: 5
  checkpoint_interval: 50000
  threaded: false
  # Signaux de récompense
  reward_signals:
    extrinsic:
      gamma: 0.99
      strength: 1.0
      network_settings:
        normalize: true
        hidden_units: 128
        num_layers: 2
        vis_encode_type: simple
        memory: null
        goal_conditioning_type: none
        deterministic: false
    # Exemple avec récompense de curiosité
    curiosity:
      gamma: 0.99
      strength: 0.02
      network_settings:
        normalize: true
        hidden_units: 128
        num_layers: 2
        vis_encode_type: simple
        memory: null
        goal_conditioning_type: none
        deterministic: false
      learning_rate: 0.0003
      encoding_size: 64

# Configuration spécifique pour un comportement
behaviors:
  # Remplacez "YourAgentName" par le nom de votre agent dans Unity
  YourAgentName:
    trainer_type: ppo
    hyperparameters:
      batch_size: 64
      buffer_size: 12000
      learning_rate: 0.0003
      beta: 0.001
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 3
      vis_encode_type: simple
      memory: null
      goal_conditioning_type: none
      deterministic: false
    # Configuration des neurones à dendrites actives
    use_active_dendrites: true
    context_size: 10
    num_dendritic_segments: 8
    segment_size: 16
    k_wta: 20
    sparsity_ratio: 0.15
    # Paramètres Synaptic Intelligence
    si_coefficient: 0.1
    # Paramètres d'entraînement
    max_steps: 1000000
    time_horizon: 64
    summary_freq: 10000
    keep_checkpoints: 5
    checkpoint_interval: 50000
    threaded: false
    # Signaux de récompense
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 128
          num_layers: 2
          vis_encode_type: simple
          memory: null
          goal_conditioning_type: none
          deterministic: false

# Configuration pour environnement multitâche
# Ajoutez plusieurs comportements pour différentes tâches
behaviors:
  Task1Agent:
    trainer_type: ppo
    use_active_dendrites: true
    context_size: 10
    num_dendritic_segments: 8
    segment_size: 16
    k_wta: 20
    sparsity_ratio: 0.15
    # ... autres paramètres identiques

  Task2Agent:
    trainer_type: ppo
    use_active_dendrites: true
    context_size: 10
    num_dendritic_segments: 8
    segment_size: 16
    k_wta: 20
    sparsity_ratio: 0.15
    # ... autres paramètres identiques

  Task3Agent:
    trainer_type: ppo
    use_active_dendrites: true
    context_size: 10
    num_dendritic_segments: 8
    segment_size: 16
    k_wta: 20
    sparsity_ratio: 0.15
    # ... autres paramètres identiques 