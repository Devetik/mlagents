# Guide d'utilisation des neurones Ã  dendrites actives dans Unity ML-Agents

Ce guide vous explique comment utiliser les neurones Ã  dendrites actives dans vos projets Unity ML-Agents.

## ðŸŽ¯ **Ã‰tapes pour utiliser les neurones Ã  dendrites actives**

### 1. **PrÃ©paration de l'environnement Unity**

#### A. Structure du projet
```
YourUnityProject/
â”œâ”€â”€ Assets/
â”‚   â”œâ”€â”€ Scripts/
â”‚   â”‚   â””â”€â”€ AgentScripts/
â”‚   â””â”€â”€ ML-Agents/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ active_dendrites_config.yaml
â””â”€â”€ models/
```

#### B. Configuration de l'agent Unity

Dans votre script d'agent Unity, ajoutez un champ pour le contexte de tÃ¢che :

```csharp
using Unity.MLAgents;
using Unity.MLAgents.Sensors;
using Unity.MLAgents.Actuators;
using UnityEngine;

public class ActiveDendritesAgent : Agent
{
    [Header("Active Dendrites Settings")]
    [SerializeField] private int taskId = 0;
    [SerializeField] private int contextSize = 10;
    
    private float[] contextVector;
    
    public override void Initialize()
    {
        base.Initialize();
        
        // Initialiser le vecteur de contexte
        contextVector = new float[contextSize];
        UpdateContextVector();
    }
    
    private void UpdateContextVector()
    {
        // CrÃ©er un vecteur one-hot pour la tÃ¢che actuelle
        for (int i = 0; i < contextSize; i++)
        {
            contextVector[i] = (i == taskId) ? 1.0f : 0.0f;
        }
    }
    
    public override void CollectObservations(VectorSensor sensor)
    {
        // Ajouter vos observations normales
        sensor.AddObservation(transform.position);
        sensor.AddObservation(transform.rotation);
        // ... autres observations
        
        // Ajouter le vecteur de contexte
        sensor.AddObservation(contextVector);
    }
    
    public void SetTask(int newTaskId)
    {
        taskId = newTaskId;
        UpdateContextVector();
    }
    
    public override void OnActionReceived(ActionBuffers actionBuffers)
    {
        // Traiter les actions reÃ§ues
        float moveX = actionBuffers.ContinuousActions[0];
        float moveZ = actionBuffers.ContinuousActions[1];
        float rotateY = actionBuffers.ContinuousActions[2];
        
        // Appliquer les actions
        transform.Translate(new Vector3(moveX, 0, moveZ) * Time.deltaTime * 5f);
        transform.Rotate(0, rotateY * Time.deltaTime * 100f, 0);
        
        // Logique de rÃ©compense
        // ...
    }
    
    public override void Heuristic(in ActionBuffers actionsOut)
    {
        // Actions manuelles pour le test
        var continuousActions = actionsOut.ContinuousActions;
        continuousActions[0] = Input.GetAxisRaw("Horizontal");
        continuousActions[1] = Input.GetAxisRaw("Vertical");
        continuousActions[2] = Input.GetAxisRaw("Mouse X");
    }
}
```

### 2. **Configuration du fichier YAML**

CrÃ©ez un fichier `configs/active_dendrites_config.yaml` :

```yaml
default_settings:
  trainer_type: ppo
  hyperparameters:
    batch_size: 64
    buffer_size: 12000
    learning_rate: 0.0003
    beta: 0.001
    epsilon: 0.2
    lambd: 0.95
    num_epoch: 3
    learning_rate_schedule: linear
  network_settings:
    normalize: true
    hidden_units: 128
    num_layers: 3
    vis_encode_type: simple
    memory: null
    goal_conditioning_type: none
    deterministic: false
  # Configuration des neurones Ã  dendrites actives
  use_active_dendrites: true
  context_size: 10
  num_dendritic_segments: 8
  segment_size: 16
  k_wta: 20
  sparsity_ratio: 0.15
  si_coefficient: 0.1
  max_steps: 1000000
  time_horizon: 64
  summary_freq: 10000
  keep_checkpoints: 5
  checkpoint_interval: 50000
  threaded: false
  reward_signals:
    extrinsic:
      gamma: 0.99
      strength: 1.0
      network_settings:
        normalize: true
        hidden_units: 128
        num_layers: 2
        vis_encode_type: simple
        memory: null
        goal_conditioning_type: none
        deterministic: false

behaviors:
  ActiveDendritesAgent:
    trainer_type: ppo
    use_active_dendrites: true
    context_size: 10
    num_dendritic_segments: 8
    segment_size: 16
    k_wta: 20
    sparsity_ratio: 0.15
    si_coefficient: 0.1
    max_steps: 1000000
    time_horizon: 64
    summary_freq: 10000
    keep_checkpoints: 5
    checkpoint_interval: 50000
    threaded: false
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 128
          num_layers: 2
          vis_encode_type: simple
          memory: null
          goal_conditioning_type: none
          deterministic: false
```

### 3. **Lancement de l'entraÃ®nement**

#### A. Via la ligne de commande
```bash
# Dans le dossier de votre projet Unity
mlagents-learn configs/active_dendrites_config.yaml --run-id=active_dendrites_experiment
```

#### B. Via Unity Editor
1. Ouvrez Unity
2. Allez dans `Window > ML-Agents > Training`
3. SÃ©lectionnez votre fichier de configuration
4. Cliquez sur "Start Training"

### 4. **Exemple d'environnement multitÃ¢che**

#### A. Script pour changer de tÃ¢che

```csharp
public class TaskManager : MonoBehaviour
{
    [SerializeField] private ActiveDendritesAgent agent;
    [SerializeField] private int currentTask = 0;
    [SerializeField] private int maxTasks = 3;
    
    public void SwitchToNextTask()
    {
        currentTask = (currentTask + 1) % maxTasks;
        agent.SetTask(currentTask);
        
        // RÃ©initialiser l'environnement pour la nouvelle tÃ¢che
        ResetEnvironmentForTask(currentTask);
    }
    
    private void ResetEnvironmentForTask(int taskId)
    {
        switch (taskId)
        {
            case 0:
                // Configuration pour la tÃ¢che 1 (ex: navigation)
                SetupNavigationTask();
                break;
            case 1:
                // Configuration pour la tÃ¢che 2 (ex: collecte d'objets)
                SetupCollectionTask();
                break;
            case 2:
                // Configuration pour la tÃ¢che 3 (ex: combat)
                SetupCombatTask();
                break;
        }
    }
    
    private void SetupNavigationTask()
    {
        // Logique pour configurer la tÃ¢che de navigation
        Debug.Log("Switching to Navigation Task");
    }
    
    private void SetupCollectionTask()
    {
        // Logique pour configurer la tÃ¢che de collecte
        Debug.Log("Switching to Collection Task");
    }
    
    private void SetupCombatTask()
    {
        // Logique pour configurer la tÃ¢che de combat
        Debug.Log("Switching to Combat Task");
    }
}
```

#### B. Configuration pour plusieurs tÃ¢ches

```yaml
behaviors:
  NavigationAgent:
    trainer_type: ppo
    use_active_dendrites: true
    context_size: 10
    num_dendritic_segments: 8
    segment_size: 16
    k_wta: 20
    sparsity_ratio: 0.15
    # ... autres paramÃ¨tres

  CollectionAgent:
    trainer_type: ppo
    use_active_dendrites: true
    context_size: 10
    num_dendritic_segments: 8
    segment_size: 16
    k_wta: 20
    sparsity_ratio: 0.15
    # ... autres paramÃ¨tres

  CombatAgent:
    trainer_type: ppo
    use_active_dendrites: true
    context_size: 10
    num_dendritic_segments: 8
    segment_size: 16
    k_wta: 20
    sparsity_ratio: 0.15
    # ... autres paramÃ¨tres
```

### 5. **Monitoring et visualisation**

#### A. Segments actifs dans TensorBoard

Les segments dendritiques actifs sont automatiquement loggÃ©s dans TensorBoard. Vous pouvez les visualiser avec :

```bash
tensorboard --logdir=results
```

#### B. Script de monitoring personnalisÃ©

```csharp
public class ActiveDendritesMonitor : MonoBehaviour
{
    [SerializeField] private ActiveDendritesAgent agent;
    
    private void Update()
    {
        // Afficher les informations de debug
        if (Input.GetKeyDown(KeyCode.D))
        {
            Debug.Log($"Current Task: {agent.GetCurrentTask()}");
            Debug.Log($"Context Vector: {string.Join(", ", agent.GetContextVector())}");
        }
    }
}
```

### 6. **Optimisation des paramÃ¨tres**

#### A. ParamÃ¨tres recommandÃ©s par type de tÃ¢che

**TÃ¢ches simples (navigation) :**
```yaml
context_size: 5
num_dendritic_segments: 4
segment_size: 8
k_wta: 10
sparsity_ratio: 0.1
```

**TÃ¢ches complexes (stratÃ©gie) :**
```yaml
context_size: 20
num_dendritic_segments: 12
segment_size: 24
k_wta: 30
sparsity_ratio: 0.2
```

**Environnements multitÃ¢che :**
```yaml
context_size: 15
num_dendritic_segments: 10
segment_size: 16
k_wta: 25
sparsity_ratio: 0.15
si_coefficient: 0.2  # Plus important pour Ã©viter l'oubli
```

#### B. Ajustement dynamique

```csharp
public class AdaptiveParameters : MonoBehaviour
{
    [SerializeField] private ActiveDendritesAgent agent;
    
    public void AdjustSparsity(float newSparsityRatio)
    {
        // Ajuster la sparsitÃ© en fonction de la performance
        agent.SetSparsityRatio(newSparsityRatio);
    }
    
    public void AdjustContextSize(int newContextSize)
    {
        // Ajuster la taille du contexte si nÃ©cessaire
        agent.SetContextSize(newContextSize);
    }
}
```

### 7. **DÃ©pannage**

#### A. ProblÃ¨mes courants

1. **Erreur "Custom neurons not available"**
   - VÃ©rifiez que les fichiers sont bien installÃ©s
   - RedÃ©marrez Unity

2. **Performance lente**
   - RÃ©duisez `num_dendritic_segments` ou `segment_size`
   - Augmentez `sparsity_ratio`

3. **Convergence lente**
   - Ajustez `learning_rate`
   - VÃ©rifiez `context_size` correspond Ã  vos observations

#### B. Debug

```csharp
public class DebugHelper : MonoBehaviour
{
    [SerializeField] private ActiveDendritesAgent agent;
    
    private void OnGUI()
    {
        GUILayout.Label($"Task ID: {agent.GetCurrentTask()}");
        GUILayout.Label($"Context Size: {agent.GetContextSize()}");
        GUILayout.Label($"Active Segments: {agent.GetActiveSegmentsCount()}");
    }
}
```

### 8. **Exemples d'utilisation avancÃ©e**

#### A. Contexte riche avec mÃ©tadonnÃ©es

```csharp
public class RichContextAgent : ActiveDendritesAgent
{
    [Header("Rich Context")]
    [SerializeField] private float difficulty = 0.5f;
    [SerializeField] private float timeOfDay = 0.0f;
    [SerializeField] private bool hasWeapon = false;
    
    protected override void UpdateContextVector()
    {
        // Contexte one-hot pour la tÃ¢che
        contextVector[0] = (taskId == 0) ? 1.0f : 0.0f;
        contextVector[1] = (taskId == 1) ? 1.0f : 0.0f;
        contextVector[2] = (taskId == 2) ? 1.0f : 0.0f;
        
        // MÃ©tadonnÃ©es riches
        contextVector[3] = difficulty;
        contextVector[4] = timeOfDay;
        contextVector[5] = hasWeapon ? 1.0f : 0.0f;
        
        // ... autres mÃ©tadonnÃ©es
    }
}
```

#### B. Continual Learning

```csharp
public class ContinualLearningManager : MonoBehaviour
{
    [SerializeField] private ActiveDendritesAgent agent;
    [SerializeField] private float taskSwitchInterval = 1000f;
    
    private float lastSwitchTime;
    private int currentTask = 0;
    
    private void Update()
    {
        if (Time.time - lastSwitchTime > taskSwitchInterval)
        {
            SwitchToNextTask();
            lastSwitchTime = Time.time;
        }
    }
    
    private void SwitchToNextTask()
    {
        currentTask = (currentTask + 1) % 3;
        agent.SetTask(currentTask);
        
        // Sauvegarder les paramÃ¨tres optimaux
        agent.SaveOptimalParameters();
    }
}
```

## ðŸŽ‰ **RÃ©sumÃ©**

Pour utiliser les neurones Ã  dendrites actives dans Unity ML-Agents :

1. âœ… **Installez les fichiers** (dÃ©jÃ  fait)
2. âœ… **Configurez votre agent** avec le vecteur de contexte
3. âœ… **CrÃ©ez le fichier YAML** avec `use_active_dendrites: true`
4. âœ… **Lancez l'entraÃ®nement** avec `mlagents-learn`
5. âœ… **Monitorez** avec TensorBoard
6. âœ… **Optimisez** les paramÃ¨tres selon vos besoins

Les neurones Ã  dendrites actives vous permettront d'avoir un apprentissage multitÃ¢che efficace avec une rÃ©tention des connaissances entre les tÃ¢ches ! 