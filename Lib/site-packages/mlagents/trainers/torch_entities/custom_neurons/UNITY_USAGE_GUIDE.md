# Guide d'utilisation des neurones √† dendrites actives dans Unity ML-Agents

Ce guide vous explique comment utiliser les neurones √† dendrites actives dans vos projets Unity ML-Agents.

## üéØ **√âtapes pour utiliser les neurones √† dendrites actives**

### 1. **Pr√©paration de l'environnement Unity**

#### A. Structure du projet
```
YourUnityProject/
‚îú‚îÄ‚îÄ Assets/
‚îÇ   ‚îú‚îÄ‚îÄ Scripts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AgentScripts/
‚îÇ   ‚îî‚îÄ‚îÄ ML-Agents/
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ active_dendrites_config.yaml
‚îî‚îÄ‚îÄ models/
```

#### B. Configuration de l'agent Unity

Dans votre script d'agent Unity, ajoutez un champ pour le contexte de t√¢che :

```csharp
using Unity.MLAgents;
using Unity.MLAgents.Sensors;
using Unity.MLAgents.Actuators;
using UnityEngine;

public class ActiveDendritesAgent : Agent
{
    [Header("Active Dendrites Settings")]
    [SerializeField] private int taskId = 0;
    [SerializeField] private int contextSize = 10;
    
    private float[] contextVector;
    
    public override void Initialize()
    {
        base.Initialize();
        
        // Initialiser le vecteur de contexte
        contextVector = new float[contextSize];
        UpdateContextVector();
    }
    
    private void UpdateContextVector()
    {
        // Cr√©er un vecteur one-hot pour la t√¢che actuelle
        for (int i = 0; i < contextSize; i++)
        {
            contextVector[i] = (i == taskId) ? 1.0f : 0.0f;
        }
    }
    
    public override void CollectObservations(VectorSensor sensor)
    {
        // Ajouter vos observations normales
        sensor.AddObservation(transform.position);
        sensor.AddObservation(transform.rotation);
        // ... autres observations
        
        // Ajouter le vecteur de contexte
        sensor.AddObservation(contextVector);
    }
    
    public void SetTask(int newTaskId)
    {
        taskId = newTaskId;
        UpdateContextVector();
    }
    
    public override void OnActionReceived(ActionBuffers actionBuffers)
    {
        // Traiter les actions re√ßues
        float moveX = actionBuffers.ContinuousActions[0];
        float moveZ = actionBuffers.ContinuousActions[1];
        float rotateY = actionBuffers.ContinuousActions[2];
        
        // Appliquer les actions
        transform.Translate(new Vector3(moveX, 0, moveZ) * Time.deltaTime * 5f);
        transform.Rotate(0, rotateY * Time.deltaTime * 100f, 0);
        
        // Logique de r√©compense
        // ...
    }
    
    public override void Heuristic(in ActionBuffers actionsOut)
    {
        // Actions manuelles pour le test
        var continuousActions = actionsOut.ContinuousActions;
        continuousActions[0] = Input.GetAxisRaw("Horizontal");
        continuousActions[1] = Input.GetAxisRaw("Vertical");
        continuousActions[2] = Input.GetAxisRaw("Mouse X");
    }
}
```

### 2. **Configuration du fichier YAML**

Cr√©ez un fichier `configs/active_dendrites_config.yaml` :

```yaml
default_settings:
  trainer_type: ppo
  hyperparameters:
    batch_size: 64
    buffer_size: 12000
    learning_rate: 0.0003
    beta: 0.001
    epsilon: 0.2
    lambd: 0.95
    num_epoch: 3
    learning_rate_schedule: linear
  network_settings:
    normalize: true
    hidden_units: 128
    num_layers: 3
    vis_encode_type: simple
    memory: null
    goal_conditioning_type: none
    deterministic: false
  # Configuration des neurones √† dendrites actives
  use_active_dendrites: true
  context_size: 10
  num_dendritic_segments: 8
  segment_size: 16
  k_wta: 20
  sparsity_ratio: 0.15
  si_coefficient: 0.1
  max_steps: 1000000
  time_horizon: 64
  summary_freq: 10000
  keep_checkpoints: 5
  checkpoint_interval: 50000
  threaded: false
  reward_signals:
    extrinsic:
      gamma: 0.99
      strength: 1.0
      network_settings:
        normalize: true
        hidden_units: 128
        num_layers: 2
        vis_encode_type: simple
        memory: null
        goal_conditioning_type: none
        deterministic: false

behaviors:
  ActiveDendritesAgent:
    trainer_type: ppo
    use_active_dendrites: true
    context_size: 10
    num_dendritic_segments: 8
    segment_size: 16
    k_wta: 20
    sparsity_ratio: 0.15
    si_coefficient: 0.1
    max_steps: 1000000
    time_horizon: 64
    summary_freq: 10000
    keep_checkpoints: 5
    checkpoint_interval: 50000
    threaded: false
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 128
          num_layers: 2
          vis_encode_type: simple
          memory: null
          goal_conditioning_type: none
          deterministic: false
```

### 3. **Lancement de l'entra√Ænement**

#### A. Via la ligne de commande
```bash
# Dans le dossier de votre projet Unity
mlagents-learn configs/active_dendrites_config.yaml --run-id=active_dendrites_experiment
```

#### B. Via Unity Editor
1. Ouvrez Unity
2. Allez dans `Window > ML-Agents > Training`
3. S√©lectionnez votre fichier de configuration
4. Cliquez sur "Start Training"

### 4. **Exemple d'environnement multit√¢che**

#### A. Script pour changer de t√¢che

```csharp
public class TaskManager : MonoBehaviour
{
    [SerializeField] private ActiveDendritesAgent agent;
    [SerializeField] private int currentTask = 0;
    [SerializeField] private int maxTasks = 3;
    
    public void SwitchToNextTask()
    {
        currentTask = (currentTask + 1) % maxTasks;
        agent.SetTask(currentTask);
        
        // R√©initialiser l'environnement pour la nouvelle t√¢che
        ResetEnvironmentForTask(currentTask);
    }
    
    private void ResetEnvironmentForTask(int taskId)
    {
        switch (taskId)
        {
            case 0:
                // Configuration pour la t√¢che 1 (ex: navigation)
                SetupNavigationTask();
                break;
            case 1:
                // Configuration pour la t√¢che 2 (ex: collecte d'objets)
                SetupCollectionTask();
                break;
            case 2:
                // Configuration pour la t√¢che 3 (ex: combat)
                SetupCombatTask();
                break;
        }
    }
    
    private void SetupNavigationTask()
    {
        // Logique pour configurer la t√¢che de navigation
        Debug.Log("Switching to Navigation Task");
    }
    
    private void SetupCollectionTask()
    {
        // Logique pour configurer la t√¢che de collecte
        Debug.Log("Switching to Collection Task");
    }
    
    private void SetupCombatTask()
    {
        // Logique pour configurer la t√¢che de combat
        Debug.Log("Switching to Combat Task");
    }
}
```

#### B. Configuration pour plusieurs t√¢ches

```yaml
behaviors:
  NavigationAgent:
    trainer_type: ppo
    use_active_dendrites: true
    context_size: 10
    num_dendritic_segments: 8
    segment_size: 16
    k_wta: 20
    sparsity_ratio: 0.15
    # ... autres param√®tres

  CollectionAgent:
    trainer_type: ppo
    use_active_dendrites: true
    context_size: 10
    num_dendritic_segments: 8
    segment_size: 16
    k_wta: 20
    sparsity_ratio: 0.15
    # ... autres param√®tres

  CombatAgent:
    trainer_type: ppo
    use_active_dendrites: true
    context_size: 10
    num_dendritic_segments: 8
    segment_size: 16
    k_wta: 20
    sparsity_ratio: 0.15
    # ... autres param√®tres
```

### 5. **Monitoring et visualisation**

#### A. Segments actifs dans TensorBoard

Les segments dendritiques actifs sont automatiquement logg√©s dans TensorBoard. Vous pouvez les visualiser avec :

```bash
tensorboard --logdir=results
```

#### B. Script de monitoring personnalis√©

```csharp
public class ActiveDendritesMonitor : MonoBehaviour
{
    [SerializeField] private ActiveDendritesAgent agent;
    
    private void Update()
    {
        // Afficher les informations de debug
        if (Input.GetKeyDown(KeyCode.D))
        {
            Debug.Log($"Current Task: {agent.GetCurrentTask()}");
            Debug.Log($"Context Vector: {string.Join(", ", agent.GetContextVector())}");
        }
    }
}
```

### 6. **Optimisation des param√®tres**

#### A. Param√®tres recommand√©s par type de t√¢che

**T√¢ches simples (navigation) :**
```yaml
context_size: 5
num_dendritic_segments: 4
segment_size: 8
k_wta: 10
sparsity_ratio: 0.1
```

**T√¢ches complexes (strat√©gie) :**
```yaml
context_size: 20
num_dendritic_segments: 12
segment_size: 24
k_wta: 30
sparsity_ratio: 0.2
```

**Environnements multit√¢che :**
```yaml
context_size: 15
num_dendritic_segments: 10
segment_size: 16
k_wta: 25
sparsity_ratio: 0.15
si_coefficient: 0.2  # Plus important pour √©viter l'oubli
```

#### B. Ajustement dynamique

```csharp
public class AdaptiveParameters : MonoBehaviour
{
    [SerializeField] private ActiveDendritesAgent agent;
    
    public void AdjustSparsity(float newSparsityRatio)
    {
        // Ajuster la sparsit√© en fonction de la performance
        agent.SetSparsityRatio(newSparsityRatio);
    }
    
    public void AdjustContextSize(int newContextSize)
    {
        // Ajuster la taille du contexte si n√©cessaire
        agent.SetContextSize(newContextSize);
    }
}
```

### 7. **D√©pannage**

#### A. Probl√®mes courants

1. **Erreur "Custom neurons not available"**
   - V√©rifiez que les fichiers sont bien install√©s
   - Red√©marrez Unity

2. **Performance lente**
   - R√©duisez `num_dendritic_segments` ou `segment_size`
   - Augmentez `sparsity_ratio`

3. **Convergence lente**
   - Ajustez `learning_rate`
   - V√©rifiez `context_size` correspond √† vos observations

#### B. Debug

```csharp
public class DebugHelper : MonoBehaviour
{
    [SerializeField] private ActiveDendritesAgent agent;
    
    private void OnGUI()
    {
        GUILayout.Label($"Task ID: {agent.GetCurrentTask()}");
        GUILayout.Label($"Context Size: {agent.GetContextSize()}");
        GUILayout.Label($"Active Segments: {agent.GetActiveSegmentsCount()}");
    }
}
```

### 8. **Exemples d'utilisation avanc√©e**

#### A. Contexte riche avec m√©tadonn√©es

```csharp
public class RichContextAgent : ActiveDendritesAgent
{
    [Header("Rich Context")]
    [SerializeField] private float difficulty = 0.5f;
    [SerializeField] private float timeOfDay = 0.0f;
    [SerializeField] private bool hasWeapon = false;
    
    protected override void UpdateContextVector()
    {
        // Contexte one-hot pour la t√¢che
        contextVector[0] = (taskId == 0) ? 1.0f : 0.0f;
        contextVector[1] = (taskId == 1) ? 1.0f : 0.0f;
        contextVector[2] = (taskId == 2) ? 1.0f : 0.0f;
        
        // M√©tadonn√©es riches
        contextVector[3] = difficulty;
        contextVector[4] = timeOfDay;
        contextVector[5] = hasWeapon ? 1.0f : 0.0f;
        
        // ... autres m√©tadonn√©es
    }
}
```

#### B. Continual Learning

```csharp
public class ContinualLearningManager : MonoBehaviour
{
    [SerializeField] private ActiveDendritesAgent agent;
    [SerializeField] private float taskSwitchInterval = 1000f;
    
    private float lastSwitchTime;
    private int currentTask = 0;
    
    private void Update()
    {
        if (Time.time - lastSwitchTime > taskSwitchInterval)
        {
            SwitchToNextTask();
            lastSwitchTime = Time.time;
        }
    }
    
    private void SwitchToNextTask()
    {
        currentTask = (currentTask + 1) % 3;
        agent.SetTask(currentTask);
        
        // Sauvegarder les param√®tres optimaux
        agent.SaveOptimalParameters();
    }
}
```

## üéâ **R√©sum√©**

Pour utiliser les neurones √† dendrites actives dans Unity ML-Agents :

1. ‚úÖ **Installez les fichiers** (d√©j√† fait)
2. ‚úÖ **Configurez votre agent** avec le vecteur de contexte
3. ‚úÖ **Cr√©ez le fichier YAML** avec `use_active_dendrites: true`
4. ‚úÖ **Lancez l'entra√Ænement** avec `mlagents-learn`
5. ‚úÖ **Monitorez** avec TensorBoard
6. ‚úÖ **Optimisez** les param√®tres selon vos besoins

Les neurones √† dendrites actives vous permettront d'avoir un apprentissage multit√¢che efficace avec une r√©tention des connaissances entre les t√¢ches ! 