"""
Intégration des neurones à dendrites actives avec ML-Agents.

Ce module fournit des classes compatibles avec l'interface ML-Agents
pour utiliser les neurones à dendrites actives dans des environnements
de reinforcement learning.
"""

import torch
import torch.nn as nn
from typing import List, Dict, Tuple, Optional, Any
import abc

from mlagents.torch_utils import torch, nn as mlagents_nn
from mlagents_envs.base_env import ActionSpec, ObservationSpec, ObservationType
from mlagents.trainers.torch_entities.action_model import ActionModel
from mlagents.trainers.torch_entities.agent_action import AgentAction
from mlagents.trainers.settings import NetworkSettings
from mlagents.trainers.torch_entities.networks import NetworkBody, ValueNetwork, Actor
from mlagents.trainers.buffer import AgentBuffer
from mlagents.trainers.torch_entities.utils import ModelUtils

from .active_dendrites import ActiveDendritesNetwork, SynapticIntelligence


class ActiveDendritesNetworkBody(NetworkBody):
    """
    Corps de réseau utilisant des neurones à dendrites actives.
    
    Compatible avec l'interface NetworkBody de ML-Agents.
    """
    
    def __init__(
        self,
        observation_specs: List[ObservationSpec],
        network_settings: NetworkSettings,
        context_size: int = 10,
        num_dendritic_segments: int = 8,
        segment_size: int = 16,
        k_wta: int = 0,
        sparsity_ratio: float = 0.1,
        encoded_act_size: int = 0,
    ):
        super().__init__(observation_specs, network_settings, encoded_act_size)
        
        # Remplacer le corps de réseau standard par notre réseau à dendrites actives
        self.context_size = context_size
        self.num_dendritic_segments = num_dendritic_segments
        self.segment_size = segment_size
        self.k_wta = k_wta
        self.sparsity_ratio = sparsity_ratio
        
        # Calculer la taille d'entrée totale
        total_enc_size = self.observation_encoder.total_enc_size + encoded_act_size
        
        # Créer le réseau à dendrites actives
        hidden_sizes = [network_settings.hidden_units] * network_settings.num_layers
        
        self.active_dendrites_network = ActiveDendritesNetwork(
            input_size=total_enc_size,
            hidden_sizes=hidden_sizes,
            output_size=network_settings.hidden_units,
            context_size=context_size,
            num_dendritic_segments=num_dendritic_segments,
            segment_size=segment_size,
            k_wta=k_wta,
            sparsity_ratio=sparsity_ratio,
            dropout_rate=0.1 if network_settings.normalize else 0.0,
            bias=True
        )
        
        # Remplacer le body encoder standard
        self._body_endoder = self.active_dendrites_network
        
        # Gestion de la mémoire LSTM si activée
        if self.use_lstm:
            # Adapter la taille de mémoire pour les dendrites actives
            self.lstm = nn.LSTM(
                network_settings.hidden_units,
                self.m_size // 2,
                batch_first=True
            )
        else:
            self.lstm = None
            
    def forward(
        self,
        inputs: List[torch.Tensor],
        actions: Optional[torch.Tensor] = None,
        memories: Optional[torch.Tensor] = None,
        sequence_length: int = 1,
        context: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass avec support du contexte pour les dendrites actives.
        
        Args:
            inputs: Observations d'entrée
            actions: Actions précédentes (optionnel)
            memories: Mémoires LSTM (optionnel)
            sequence_length: Longueur de séquence pour LSTM
            context: Vecteur de contexte pour les dendrites actives
            
        Returns:
            Tuple (encoding, memories)
        """
        # Encoder les observations
        encoded_self = self.observation_encoder(inputs)
        
        # Ajouter les actions si fournies
        if actions is not None:
            encoded_self = torch.cat([encoded_self, actions], dim=1)
            
        # Gérer le conditionnement par objectif si activé
        if isinstance(self._body_endoder, ActiveDendritesNetwork):
            # Utiliser le réseau à dendrites actives
            if context is None:
                # Créer un contexte par défaut si non fourni
                batch_size = encoded_self.shape[0]
                context = torch.zeros(batch_size, self.context_size, device=encoded_self.device)
                
            encoding = self._body_endoder(encoded_self, context)
        else:
            # Fallback vers le comportement standard
            if hasattr(self._body_endoder, 'get_goal_encoding'):
                goal = self.observation_encoder.get_goal_encoding(inputs)
                encoding = self._body_endoder(encoded_self, goal)
            else:
                encoding = self._body_endoder(encoded_self)
                
        # Gestion LSTM si activée
        if self.use_lstm:
            encoding = encoding.reshape([-1, sequence_length, self.h_size])
            encoding, memories = self.lstm(encoding, memories)
            encoding = encoding.reshape([-1, self.m_size // 2])
            
        return encoding, memories
    
    def get_active_segments(self) -> List[List[torch.Tensor]]:
        """Retourne les segments actifs du réseau à dendrites actives."""
        if hasattr(self._body_endoder, 'get_active_segments'):
            return self._body_endoder.get_active_segments()
        return []
    
    def update_importance_weights(self, loss: torch.Tensor):
        """Met à jour les coefficients d'importance SI."""
        if hasattr(self._body_endoder, 'update_importance_weights'):
            self._body_endoder.update_importance_weights(loss)
    
    def save_optimal_params(self):
        """Sauvegarde les paramètres optimaux pour SI."""
        if hasattr(self._body_endoder, 'save_optimal_params'):
            self._body_endoder.save_optimal_params()
    
    def compute_regularization_loss(self) -> torch.Tensor:
        """Calcule le terme de régularisation SI."""
        if hasattr(self._body_endoder, 'compute_regularization_loss'):
            return self._body_endoder.compute_regularization_loss()
        return torch.tensor(0.0, device=self._body_endoder.layers[0].neurons[0].feedforward_weight.device)


class ActiveDendritesValueNetwork(ValueNetwork):
    """
    Réseau de valeur utilisant des neurones à dendrites actives.
    
    Compatible avec l'interface ValueNetwork de ML-Agents.
    """
    
    def __init__(
        self,
        stream_names: List[str],
        observation_specs: List[ObservationSpec],
        network_settings: NetworkSettings,
        context_size: int = 10,
        num_dendritic_segments: int = 8,
        segment_size: int = 16,
        k_wta: int = 0,
        sparsity_ratio: float = 0.1,
        encoded_act_size: int = 0,
        outputs_per_stream: int = 1,
    ):
        # Initialiser la classe parent
        nn.Module.__init__(self)
        
        # Remplacer le network_body par notre version à dendrites actives
        self.network_body = ActiveDendritesNetworkBody(
            observation_specs,
            network_settings,
            context_size,
            num_dendritic_segments,
            segment_size,
            k_wta,
            sparsity_ratio,
            encoded_act_size
        )
        
        # Déterminer la taille d'encodage
        if network_settings.memory is not None:
            encoding_size = network_settings.memory.memory_size // 2
        else:
            encoding_size = network_settings.hidden_units
            
        # Créer les têtes de valeur
        from mlagents.trainers.torch_entities.decoders import ValueHeads
        self.value_heads = ValueHeads(stream_names, encoding_size, outputs_per_stream)
        
    def critic_pass(
        self,
        inputs: List[torch.Tensor],
        memories: Optional[torch.Tensor] = None,
        sequence_length: int = 1,
        context: Optional[torch.Tensor] = None,
    ) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:
        """
        Passage critique avec support du contexte.
        
        Args:
            inputs: Observations d'entrée
            memories: Mémoires LSTM (optionnel)
            sequence_length: Longueur de séquence
            context: Vecteur de contexte pour les dendrites actives
            
        Returns:
            Tuple (value_outputs, critic_mem_out)
        """
        value_outputs, critic_mem_out = self.forward(
            inputs, memories=memories, sequence_length=sequence_length, context=context
        )
        return value_outputs, critic_mem_out
    
    def forward(
        self,
        inputs: List[torch.Tensor],
        actions: Optional[torch.Tensor] = None,
        memories: Optional[torch.Tensor] = None,
        sequence_length: int = 1,
        context: Optional[torch.Tensor] = None,
    ) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:
        """
        Forward pass avec support du contexte.
        
        Args:
            inputs: Observations d'entrée
            actions: Actions (optionnel)
            memories: Mémoires LSTM (optionnel)
            sequence_length: Longueur de séquence
            context: Vecteur de contexte pour les dendrites actives
            
        Returns:
            Tuple (output, memories)
        """
        encoding, memories = self.network_body(
            inputs, actions, memories, sequence_length, context
        )
        output = self.value_heads(encoding)
        return output, memories


class ActiveDendritesActor(nn.Module, Actor):
    """
    Acteur utilisant des neurones à dendrites actives.
    
    Compatible avec l'interface Actor de ML-Agents.
    """
    
    def __init__(
        self,
        observation_specs: List[ObservationSpec],
        network_settings: NetworkSettings,
        action_spec: ActionSpec,
        context_size: int = 10,
        num_dendritic_segments: int = 8,
        segment_size: int = 16,
        k_wta: int = 0,
        sparsity_ratio: float = 0.1,
        conditional_sigma: bool = False,
        tanh_squash: bool = False,
    ):
        super().__init__()
        self.action_spec = action_spec
        
        # Créer le corps de réseau à dendrites actives
        self.network_body = ActiveDendritesNetworkBody(
            observation_specs,
            network_settings,
            context_size,
            num_dendritic_segments,
            segment_size,
            k_wta,
            sparsity_ratio
        )
        
        # Déterminer la taille d'encodage
        if network_settings.memory is not None:
            self.encoding_size = network_settings.memory.memory_size // 2
        else:
            self.encoding_size = network_settings.hidden_units
            
        # Créer le modèle d'action
        self.action_model = ActionModel(
            self.encoding_size,
            action_spec,
            conditional_sigma=conditional_sigma,
            tanh_squash=tanh_squash,
            deterministic=network_settings.deterministic,
        )
        
    def update_normalization(self, buffer: AgentBuffer) -> None:
        """Met à jour la normalisation basée sur le buffer."""
        self.network_body.update_normalization(buffer)
        
    @property
    def memory_size(self) -> int:
        """Retourne la taille de mémoire du réseau."""
        return self.network_body.memory_size
        
    def get_action_and_stats(
        self,
        inputs: List[torch.Tensor],
        masks: Optional[torch.Tensor] = None,
        memories: Optional[torch.Tensor] = None,
        sequence_length: int = 1,
        context: Optional[torch.Tensor] = None,
    ) -> Tuple[AgentAction, Dict[str, Any], torch.Tensor]:
        """
        Obtient l'action et les statistiques avec support du contexte.
        
        Args:
            inputs: Observations d'entrée
            masks: Masques d'action (optionnel)
            memories: Mémoires LSTM (optionnel)
            sequence_length: Longueur de séquence
            context: Vecteur de contexte pour les dendrites actives
            
        Returns:
            Tuple (action, stats, memories)
        """
        encoding, memories = self.network_body(
            inputs, memories=memories, sequence_length=sequence_length, context=context
        )
        
        action, log_probs, entropies = self.action_model(encoding, masks)
        
        stats = {
            "log_probs": log_probs,
            "entropies": entropies,
        }
        
        return action, stats, memories
        
    def get_stats(
        self,
        inputs: List[torch.Tensor],
        actions: AgentAction,
        masks: Optional[torch.Tensor] = None,
        memories: Optional[torch.Tensor] = None,
        sequence_length: int = 1,
        context: Optional[torch.Tensor] = None,
    ) -> Dict[str, Any]:
        """
        Obtient les statistiques pour des actions données.
        
        Args:
            inputs: Observations d'entrée
            actions: Actions à évaluer
            masks: Masques d'action (optionnel)
            memories: Mémoires LSTM (optionnel)
            sequence_length: Longueur de séquence
            context: Vecteur de contexte pour les dendrites actives
            
        Returns:
            Dictionnaire des statistiques
        """
        encoding, memories = self.network_body(
            inputs, memories=memories, sequence_length=sequence_length, context=context
        )
        
        log_probs, entropies = self.action_model.evaluate(encoding, actions, masks)
        
        return {
            "log_probs": log_probs,
            "entropies": entropies,
        }
    
    def get_active_segments(self) -> List[List[torch.Tensor]]:
        """Retourne les segments actifs du réseau."""
        return self.network_body.get_active_segments()
    
    def update_importance_weights(self, loss: torch.Tensor):
        """Met à jour les coefficients d'importance SI."""
        self.network_body.update_importance_weights(loss)
    
    def save_optimal_params(self):
        """Sauvegarde les paramètres optimaux pour SI."""
        self.network_body.save_optimal_params()
    
    def compute_regularization_loss(self) -> torch.Tensor:
        """Calcule le terme de régularisation SI."""
        return self.network_body.compute_regularization_loss()


class ContextProvider:
    """
    Fournisseur de contexte pour les neurones à dendrites actives.
    
    Cette classe aide à gérer le contexte pour différentes tâches
    dans un environnement multitâche ou séquentiel.
    """
    
    def __init__(self, context_size: int, num_tasks: int = 10):
        self.context_size = context_size
        self.num_tasks = num_tasks
        
        # Créer des vecteurs de contexte one-hot pour chaque tâche
        self.task_contexts = torch.eye(num_tasks, context_size)
        
        # Contexte actuel
        self.current_task_id = 0
        
    def get_context(self, batch_size: int, task_id: Optional[int] = None) -> torch.Tensor:
        """
        Obtient le vecteur de contexte pour une tâche donnée.
        
        Args:
            batch_size: Taille du batch
            task_id: ID de la tâche (si None, utilise la tâche actuelle)
            
        Returns:
            Vecteur de contexte (batch_size, context_size)
        """
        if task_id is None:
            task_id = self.current_task_id
            
        task_id = task_id % self.num_tasks
        
        # Obtenir le contexte pour cette tâche
        context = self.task_contexts[task_id].unsqueeze(0)
        
        # Répéter pour le batch
        context = context.repeat(batch_size, 1)
        
        return context
    
    def set_current_task(self, task_id: int):
        """Définit la tâche actuelle."""
        self.current_task_id = task_id % self.num_tasks
    
    def get_task_contexts(self) -> torch.Tensor:
        """Retourne tous les vecteurs de contexte de tâches."""
        return self.task_contexts.clone() 