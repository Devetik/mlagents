# Neurones √† Dendrites Actives pour ML-Agents

Ce module impl√©mente des neurones √† dendrites actives avanc√©s pour ML-Agents, permettant l'apprentissage multit√¢che et le continual learning dans des environnements de reinforcement learning.

## üß† Fonctionnalit√©s

### 1. Neurones √† Dendrites Actives (Active Dendrites Neuron)

Chaque neurone re√ßoit :
- **Entr√©e feedforward classique** : √©tat de l'agent
- **Vecteur de contexte** : description de la t√¢che actuelle (one-hot)

L'activation est calcul√©e comme :
```
tÃÇ = w·µóx + b (feedforward)
d = max_j(u_j·µó c) (modulation dendritique)
output = tÃÇ * sigmoid(d)
```

Seul le segment dendritique le plus actif est mis √† jour pour chaque neurone.

### 2. k-Winner-Take-All (kWTA)

M√©canisme de sparsit√© qui :
- S√©lectionne les k activations les plus √©lev√©es
- Met les autres √† z√©ro
- Force la sparsit√© des repr√©sentations

### 3. Synaptic Intelligence (SI)

Gestion de l'importance synaptique pour le continual learning :
- Chaque param√®tre a un coefficient d'importance œâ
- Mise √† jour dynamique pendant l'apprentissage
- Terme de r√©gularisation : `L_total = L_task + c * ‚àë œâ_i * (Œ∏_i - Œ∏*_i)¬≤`

## üìÅ Structure du Module

```
custom_neurons/
‚îú‚îÄ‚îÄ __init__.py                 # Exports principaux
‚îú‚îÄ‚îÄ active_dendrites.py         # Impl√©mentation des neurones
‚îú‚îÄ‚îÄ mlagents_integration.py     # Int√©gration ML-Agents
‚îú‚îÄ‚îÄ example_usage.py           # Exemples d'utilisation
‚îî‚îÄ‚îÄ README.md                  # Documentation
```

## üöÄ Utilisation Rapide

### Installation

Le module est d√©j√† int√©gr√© dans ML-Agents. Importez-le directement :

```python
from mlagents.trainers.torch_entities.custom_neurons import (
    ActiveDendritesActor,
    ActiveDendritesValueNetwork,
    ContextProvider
)
```

### Exemple Basique

```python
import torch
from mlagents_envs.base_env import ActionSpec, ObservationSpec, ObservationType
from mlagents.trainers.settings import NetworkSettings
from mlagents.trainers.torch_entities.custom_neurons import (
    ActiveDendritesActor,
    ContextProvider
)

# Cr√©er les sp√©cifications
observation_specs = [
    ObservationSpec(shape=(10,), observation_type=ObservationType.DEFAULT)
]
action_spec = ActionSpec(continuous_size=4, discrete_branches=[])

# Param√®tres du r√©seau
network_settings = NetworkSettings(
    hidden_units=128,
    num_layers=3,
    normalize=True
)

# Cr√©er l'acteur avec dendrites actives
actor = ActiveDendritesActor(
    observation_specs=observation_specs,
    network_settings=network_settings,
    action_spec=action_spec,
    context_size=10,
    num_dendritic_segments=8,
    segment_size=16,
    k_wta=20,
    sparsity_ratio=0.15
)

# Fournisseur de contexte
context_provider = ContextProvider(context_size=10, num_tasks=5)

# Utilisation
batch_size = 4
inputs = [torch.randn(batch_size, 10)]
context = context_provider.get_context(batch_size, task_id=0)

action, stats, memories = actor.get_action_and_stats(inputs, context=context)
```

## ‚öôÔ∏è Param√®tres

### ActiveDendritesActor/ValueNetwork

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `context_size` | int | 10 | Taille du vecteur de contexte |
| `num_dendritic_segments` | int | 8 | Nombre de segments dendritiques par neurone |
| `segment_size` | int | 16 | Taille de chaque segment dendritique |
| `k_wta` | int | 0 | Nombre de neurones actifs (0 = automatique) |
| `sparsity_ratio` | float | 0.1 | Ratio de sparsit√© si k_wta=0 |

### ContextProvider

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `context_size` | int | 10 | Taille du vecteur de contexte |
| `num_tasks` | int | 10 | Nombre de t√¢ches support√©es |

## üî¨ Continual Learning

### Exemple avec Synaptic Intelligence

```python
import torch.optim as optim

# Cr√©er l'acteur et l'optimiseur
actor = ActiveDendritesActor(...)
optimizer = optim.Adam(actor.parameters(), lr=1e-4)

# Apprentissage sur plusieurs t√¢ches
for task_id in range(num_tasks):
    # Sauvegarder les param√®tres optimaux
    actor.save_optimal_params()
    
    for episode in range(episodes_per_task):
        # Forward pass
        action, stats, memories = actor.get_action_and_stats(inputs, context)
        
        # Calculer la perte
        loss = compute_rl_loss(action, stats)
        
        # Mettre √† jour les coefficients d'importance SI
        actor.update_importance_weights(loss)
        
        # Ajouter la r√©gularisation SI
        si_loss = actor.compute_regularization_loss()
        total_loss = loss + 0.1 * si_loss
        
        # Backward pass
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
```

## üìä Visualisation

### Segments Actifs

```python
# Obtenir les segments actifs
active_segments = actor.get_active_segments()

# Analyser par couche
for layer_idx, layer_segments in enumerate(active_segments):
    layer_active = sum(torch.sum(seg).item() for seg in layer_segments)
    print(f"Couche {layer_idx}: {layer_active} segments actifs")
```

### Contexte par T√¢che

```python
# Obtenir tous les contextes de t√¢ches
task_contexts = context_provider.get_task_contexts()
print(f"Contexte t√¢che 0: {task_contexts[0]}")
```

## üß™ Tests et Validation

Ex√©cutez les exemples inclus :

```python
from mlagents.trainers.torch_entities.custom_neurons.example_usage import *

# Exemple basique
example_basic_usage()

# Exemple Actor-Critic
example_actor_critic()

# Exemple de continual learning
example_continual_learning()

# Exemple de visualisation
example_visualization()
```

## üîß Int√©gration Avanc√©e

### Personnalisation des Param√®tres

```python
# R√©seau personnalis√©
actor = ActiveDendritesActor(
    observation_specs=observation_specs,
    network_settings=network_settings,
    action_spec=action_spec,
    context_size=20,           # Plus de contexte
    num_dendritic_segments=12, # Plus de segments
    segment_size=24,           # Segments plus grands
    k_wta=30,                  # Plus de neurones actifs
    sparsity_ratio=0.2         # Plus de sparsit√©
)
```

### Contexte Personnalis√©

```python
# Cr√©er un contexte personnalis√©
custom_context = torch.tensor([
    [1, 0, 0, 0, 0, 0.5, 0.3, 0.2, 0.1, 0.0],  # T√¢che 0 avec m√©tadonn√©es
    [0, 1, 0, 0, 0, 0.2, 0.8, 0.1, 0.0, 0.0],  # T√¢che 1 avec m√©tadonn√©es
])

# Utiliser le contexte personnalis√©
action, stats, memories = actor.get_action_and_stats(
    inputs, context=custom_context
)
```

## üìö R√©f√©rences

Cette impl√©mentation est bas√©e sur :

1. **Neurones √† Dendrites Actives** : Travaux sur la modulation dendritique dans les r√©seaux de neurones
2. **Synaptic Intelligence** : Zenke et al. (2017) - Continual Learning Through Synaptic Intelligence
3. **k-Winner-Take-All** : M√©canismes de sparsit√© pour l'efficacit√© computationnelle

## üêõ D√©pannage

### Probl√®mes Courants

1. **Erreur de dimension** : V√©rifiez que `context_size` correspond √† la taille du contexte fourni
2. **M√©moire insuffisante** : R√©duisez `num_dendritic_segments` ou `segment_size`
3. **Convergence lente** : Ajustez `sparsity_ratio` ou `k_wta`

### Debug

```python
# V√©rifier les dimensions
print(f"Taille d'entr√©e: {inputs[0].shape}")
print(f"Taille de contexte: {context.shape}")
print(f"Taille de sortie: {action.continuous_tensor.shape}")

# V√©rifier les segments actifs
active_segments = actor.get_active_segments()
print(f"Segments actifs: {len(active_segments)} couches")
```

## ü§ù Contribution

Pour contribuer √† ce module :

1. Testez avec diff√©rents environnements
2. Optimisez les hyperparam√®tres
3. Ajoutez de nouvelles fonctionnalit√©s
4. Documentez les am√©liorations

## üìÑ Licence

Ce module fait partie de ML-Agents et suit la m√™me licence. 